{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d41991-3e59-4587-9904-a43131e20a7d",
   "metadata": {},
   "source": [
    "# TODOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a07b0-8465-4a55-9dc2-ef3a92622f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown('TODO.md'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cefb50",
   "metadata": {},
   "source": [
    "# Design Pattern Recognition with Software Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dd595e-942a-4e01-994d-0c425619a9d8",
   "metadata": {},
   "source": [
    "## Library/Package Imports\n",
    "All required modules should be in the next cell to avoid scattered imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3867f5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore missing imports warnings in vs code\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from typing import Callable\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from typing import Optional, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65910d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common utility functions\n",
    "def get_metric_columns():\n",
    "    return ['NOF', 'NSF', 'NOM', 'NSM', 'NOAM', 'NORM', 'NOPC', 'NOOF', 'NCOF']\n",
    "\n",
    "def get_ck_metric_columns():\n",
    "    return ['CBO', 'FAN_IN', 'FAN_OUT']\n",
    "\n",
    "def get_feature_columns():\n",
    "    return get_metric_columns() + get_ck_metric_columns() + ['role_kind']\n",
    "\n",
    "def get_label_column():\n",
    "    return ['role']\n",
    "\n",
    "def generate_subplot(df: pd.DataFrame, plot_func: Callable[[pd.DataFrame, str], go.Figure], subplot_width: int = 600, subplot_height: int = 2400) -> go.Figure:\n",
    "    metric_columns = get_metric_columns() + get_ck_metric_columns()\n",
    "    subplots = make_subplots(\n",
    "        len(metric_columns), subplot_titles=metric_columns)\n",
    "    for i, metric in enumerate(metric_columns):\n",
    "        figure = plot_func(df, metric)\n",
    "        subplots.add_trace(figure, row=i+1, col=1)\n",
    "    subplots['layout'].update(height=subplot_height, width=subplot_width)\n",
    "    return subplots\n",
    "\n",
    "\n",
    "def generate_selectable_graph_for_metrics(df: pd.DataFrame, initial_plot_func: Callable[[], go.Figure], update_func: Callable[[go.Figure, pd.DataFrame, str], None], y_label: Optional[str] = None):\n",
    "    metric_dropdown = widgets.Dropdown(options=get_metric_columns() + get_ck_metric_columns())\n",
    "    fig = go.FigureWidget(initial_plot_func())\n",
    "\n",
    "    def on_metric_changed(change):\n",
    "        metric = change['new']\n",
    "        with fig.batch_update():\n",
    "            figure = fig.data[0]\n",
    "            update_func(figure, df, metric)\n",
    "            figure['name'] = metric\n",
    "            label = y_label if y_label else ' '\n",
    "            fig.update_layout(title=metric, bargap=0.5,\n",
    "                              xaxis_title=metric, yaxis_title=label)\n",
    "\n",
    "    metric_dropdown.observe(on_metric_changed, names='value')\n",
    "    display(widgets.VBox([metric_dropdown, fig]))\n",
    "\n",
    "\n",
    "def filter_extended_projects(df: pd.DataFrame):\n",
    "    return df[~df['project'].str.contains('DrJava')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef9395",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generation of metrics\n",
    "\n",
    "If the metrics are not yet generated, the following steps are required:\n",
    "\n",
    "1. Make sure that `source_files.zip` is located in the current directory. The archive contains the actual zipped source code of the projects in [P-MArT](https://www.ptidej.net/tools/designpatterns/) and `pmart.xml` with descriptions of the micro architectures\n",
    "2. Create a new virtual Python environment with `python -m venv .` in the current directory if not yet done\n",
    "3. Activate the virtual environment ([refer here for the actual command to run](https://docs.python.org/3/library/venv.html#how-venvs-work))\n",
    "4. Execute `python3 preprocess_source_files.py` to extract the source files from `source_files.zip` and move the source files described in `pmart.xml` into `dataset` directory. For more information run `python3 preprocess_source_files.py -h`.\n",
    "    - Source files are structured as `<dataset_dir>/<design_pattern/micro_architecture_<id>`\n",
    "    - Each micro architecture directory contains the following files:\n",
    "        - `roles.csv`: Roles, entity names and role kind as described in `pmart.xml`\n",
    "        - `projects.txt`: From which project the source files come from\n",
    "        - The source files to be evaluated\n",
    "5. \n",
    "    - **OLD**: Execute `python3 generate_source_file_metrics.py` to generate `metrics.csv`. For more information run `python3 generate_source_file_metrics.py`.\n",
    "    - **NEW**: Execute `docker build --file docker/sourcefileparser.dockerfile . -t sourcefilerparser:latest` in the `project` directory to build the tool and run `docker run -v ./:/home/app/volume  -e DATASET_PATH=./dataset -e OUTPUT_CSV=./m.csv sourcefilerparser:latest` for metric generation\n",
    "\n",
    "**NOTES**: \n",
    "- As the projects in this dataset are old and not all projects listed in P-MaRT are not accessible, some source files and their entries in `metrics` may be missing.\n",
    "- The tool for generating the metrics was originally written with a Java Parser implemented Python only. This lead to parsing issues in some source files. As a result, the tool was rewritten as a Java project with a native parser. The original Python script is included for completeness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33566ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Overview about `metrics.csv`\n",
    "\n",
    "In order to detect applied Gang Of Four design patterns in source code with machine learning strategies, we first need to transform the source file into a numerical representation that can be understood by a machine learning model.\n",
    "This approach aims to solve this by generating numerical characteristics for each source file in the context of the regarded micro architecture. As there are several methods to define what metrics to include in the evaluation, the metrics as described [in this paper](../sources/JSEA-DP-2014.pdf):\n",
    "\n",
    "- NOF: Number of fields\n",
    "- NSF: Number of static fields\n",
    "- NOM: Number of methods\n",
    "- NSM: Number of static methods\n",
    "- NOAM: Number of abstract methods\n",
    "- NORM: Number of overridden methods\n",
    "- NOPC: Number of private constrcutors\n",
    "- NOOF: Number of object fields\n",
    "- NCOF: Number of other classes with field of own type\n",
    "\n",
    "\n",
    "In addition to these metrics, the following Chidamber & Kemerer object-oriented metrics were added to quantify the relation, coupling and cohesion between participants in a design pattern:\n",
    "\n",
    "- FAN_IN: Number of input dependencies\n",
    "- FAN_OUT: Number of output dependencies\n",
    "- CBO: Coupling between objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ab6a56-c832-4e2d-93d5-ad5ea97e5bf8",
   "metadata": {},
   "source": [
    "## Outlier Detection and Removal\n",
    "\n",
    "As the dataset may contain a varied implementation of datasets, outlier detection and removal may be required to reduce the noise in the dataset. `sklearn` provides the some automatic and unsupervised approaches out of the box. The following are considered\n",
    "\n",
    "**NOTE**: This list is subject to change\n",
    "\n",
    "* Isolation Forest\n",
    "* Local Outlier Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a743b-1abd-41fc-8d74-837e10a5e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports for this section\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a025d56",
   "metadata": {},
   "source": [
    "### Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1daa46b-d8c0-4008-95ac-97820f595da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_isolation_forest(df: pd.DataFrame):\n",
    "    df_filtered = df.copy()\n",
    "    isolation_forest = IsolationForest(contamination=0.1)\n",
    "    df_filtered['outlier'] = isolation_forest.fit_predict(\n",
    "        df_filtered[get_metric_columns()])\n",
    "    df_filtered = df_filtered[df_filtered['outlier'] == 1]\n",
    "    return df_filtered.drop(columns=['outlier'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d4fcca",
   "metadata": {},
   "source": [
    "### Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34133435-52f9-42a7-824f-9fe1aaf96183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_local_outlier_factor(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    threshold = 0\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    lof = LocalOutlierFactor(contamination=0.5)\n",
    "    df_copy['outlier_score'] = lof.fit_predict(df_copy[get_metric_columns()])\n",
    "    df_copy = df_copy[df_copy['outlier_score'] > threshold]\n",
    "    return df_copy.drop(columns=('outlier_score'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f390ae",
   "metadata": {},
   "source": [
    "## Explorative Data Analysis of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c7b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./metrics_ck.csv')\n",
    "df = filter_extended_projects(df)\n",
    "df = apply_isolation_forest(df)\n",
    "print(f'{df.shape[0]} rows were imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d63a37-a043-43ba-aca0-693e0e6ccabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['role'] = df['role'].str.lower().str.strip()\n",
    "df['role_kind'] = df['role_kind'].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9d1680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if columns in dataframe have expected types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f534e36",
   "metadata": {},
   "source": [
    "### Filter Dataframe entries by micro architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24e61d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_arches = df['micro_architecture'].unique().tolist()\n",
    "\n",
    "\n",
    "def view(micro_arch=''):\n",
    "    display(df[df['micro_architecture'] == micro_arch]\n",
    "            [['role', 'role_kind', 'entity'] + get_metric_columns() + get_ck_metric_columns()], clear=True)\n",
    "\n",
    "\n",
    "w = widgets.Dropdown(options=micro_arches)\n",
    "widgets.interactive(view, micro_arch=w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b016f497",
   "metadata": {},
   "source": [
    "### Distribution of roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79959eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "px.histogram(df, x='role', histfunc='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240b2766-9c37-4c29-a2d8-cee945d1ba17",
   "metadata": {},
   "source": [
    "### Distribution of design patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f654cb14-24d4-403d-ad90-7db5bbf0e1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binned_by_micro_arch_dp = df.copy()\n",
    "df_binned_by_micro_arch_dp = df_binned_by_micro_arch_dp.drop_duplicates(\n",
    "    ['micro_architecture', 'design_pattern']).reset_index()\n",
    "df_binned_by_micro_arch_dp = df_binned_by_micro_arch_dp['design_pattern'].value_counts(\n",
    ").reset_index()\n",
    "\n",
    "fig = px.histogram(df_binned_by_micro_arch_dp, x='design_pattern', y='count')\n",
    "fig.update_layout(xaxis_title='Design Pattern',\n",
    "                  yaxis_title='Count in Design Pattern')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca83e997",
   "metadata": {},
   "source": [
    "### Distribution for metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236b326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_histogram():\n",
    "    return go.Histogram(\n",
    "        histfunc='count',\n",
    "    )\n",
    "\n",
    "\n",
    "def update_histogram(figure: go.Figure, df: pd.DataFrame, metric: str):\n",
    "    figure['x'] = df[metric]\n",
    "\n",
    "\n",
    "generate_selectable_graph_for_metrics(\n",
    "    df, initial_histogram, update_histogram, 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e5650",
   "metadata": {},
   "source": [
    "### Box Plots for metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51e3449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_histogram():\n",
    "    return go.Box(\n",
    "    )\n",
    "\n",
    "\n",
    "def update_histogram(figure: go.Figure, df: pd.DataFrame, metric: str):\n",
    "    figure['x'] = df[metric]\n",
    "\n",
    "\n",
    "generate_selectable_graph_for_metrics(df, initial_histogram, update_histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa3c25",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "As design patterns can be considered as small scale appliances of software architecture, they consist of different entities with different relationships and roles to fulfill in the regarded design pattern. In order to detect design patterns, we first need to detect what kind of role a given Java class or entity it most likely corresponds to. To achieve this, machine learning model capable of classifying multiple labels should be considered. The extracted software metrics are the numerical inputs and the most likely roles in a design pattern are the result. \n",
    "As this falls in the area of supervised machine learning, initially the following models/techniques are to be considered:\n",
    "\n",
    "**NOTE:** This list is subject to change \n",
    "\n",
    "* Support Vector Machines\n",
    "* Tree Classifiers\n",
    "* Ensemble Classifiers (e.g Random Forest Classifier)\n",
    "* Custom Convoluted Network\n",
    "\n",
    "In order to optimize the given results of a given model, first RandomGridSearch is applied to determine a range of values or selection for the hyperparameters while GridSearch is used to determine the most optimal available value or selection for the regarded hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edebf2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required import for machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import hpsklearn\n",
    "import hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065cf01b-0f73-41ad-bab3-fdffdd3775e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared functions in this section\n",
    "def label_encoder():\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    def encode(col: pd.Series):\n",
    "        return label_encoder.fit_transform(col)\n",
    "\n",
    "    def decode(encoded_col: pd.Series):\n",
    "        return label_encoder.inverse_transform(encoded_col)\n",
    "\n",
    "    return encode, decode\n",
    "\n",
    "\n",
    "def preprocess(df: pd.DataFrame):\n",
    "    df_copy = df.copy()\n",
    "    role_kind_encode, role_kind_decode = label_encoder()\n",
    "    role_encode, role_decode = label_encoder()\n",
    "    df_copy['role_kind'] = role_kind_encode(df_copy['role_kind'])\n",
    "    df_copy['role'] = role_encode(df['role'])\n",
    "    train, test = train_test_split(\n",
    "        df_copy[get_feature_columns() + get_label_column()], test_size=0.25)\n",
    "    return (train, test, {'role_kind': role_kind_decode, 'role': role_decode})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcbce6f-6f9a-453a-9a32-37634cf3e9eb",
   "metadata": {},
   "source": [
    "### Prepare Datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f0e842-285d-46ef-9410-6c14fe3d650f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Limit design patterns to Top 3 with most samples\n",
    "df_dataset = df[(df['design_pattern'] == 'adapter') | (df['design_pattern'] == 'observer') | (df['design_pattern'] == 'abstract_factory')]\n",
    "df_train, df_test, decoder = preprocess(df_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7df3789-9e1f-4ae3-8efd-6606839eaad0",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006c8ba1-fb69-4239-8f78-835b8a383c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_svm(df_train: pd.DataFrame, df_test: pd.DataFrame):\n",
    "    X_train = df_train[get_feature_columns()]\n",
    "    y_train = df_train[get_label_column()].values.ravel()\n",
    "\n",
    "    X_test = df_test[get_feature_columns()]\n",
    "    y_test = df_test[get_label_column()].values.ravel()\n",
    "\n",
    "    standard_scaler = StandardScaler()\n",
    "    X_train = standard_scaler.fit_transform(X_train)\n",
    "    X_test = standard_scaler.fit_transform(X_test)\n",
    "\n",
    "    svm_classifier = svm.SVC(kernel='rbf', gamma=0.1, C=1.75)\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "    pred = svm_classifier.predict(X_test)\n",
    "    # print(f1_score(y_true=y_test, y_pred=pred, average='weighted'))\n",
    "    return svm_classifier.score(X_test, y_test)\n",
    "\n",
    "\n",
    "apply_svm(df_train, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0516a563-d27b-401d-ae1c-16bc4528c458",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395b288b-1501-4262-9a99-2711a575581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_random_forest(df_train: pd.DataFrame, df_test: pd.DataFrame):\n",
    "    X_train = df_train[get_feature_columns()]\n",
    "    y_train = df_train[get_label_column()].values.ravel()\n",
    "\n",
    "    X_test = df_test[get_feature_columns()]\n",
    "    y_test = df_test[get_label_column()].values.ravel()\n",
    "\n",
    "    random_forest_classifier = RandomForestClassifier(\n",
    "        max_depth=30, random_state=1)\n",
    "    random_forest_classifier.fit(X_train, y_train)\n",
    "\n",
    "    pred = random_forest_classifier.predict(X_test)\n",
    "    # print(f1_score(y_true=y_test, y_pred=pred, average='weighted'))\n",
    "    return random_forest_classifier.score(X_test, y_test)\n",
    "\n",
    "\n",
    "apply_random_forest(df_train, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aa5f3c-7044-4840-9c10-b6a11691a5f0",
   "metadata": {},
   "source": [
    "### Get Best Possible Classifier with hyperopt-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c407fe-985c-48c2-bbb7-8e5fe37f959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_hyperopt(df_train: pd.DataFrame, df_test: pd.DataFrame):\n",
    "    X_train = df_train[get_feature_columns()]\n",
    "    y_train = df_train[get_label_column()].values.ravel()\n",
    "\n",
    "    X_test = df_test[get_feature_columns()]\n",
    "    y_test = df_test[get_label_column()].values.ravel()\n",
    "    chosen_classifiers = [\n",
    "        #TODO\n",
    "    ]\n",
    "\n",
    "    p = 1 / len(chosen_classifiers)\n",
    "    classifiers = hp.pchoice('cls', [(p, c) for c in chosen_classifiers])\n",
    "\n",
    "    hyper_estimator = hpsklearn.HyperoptEstimator(\n",
    "        classifier=classifiers,\n",
    "        preprocessing=[],\n",
    "        max_evals=20,\n",
    "        algo=hyperopt.tpe.suggest,\n",
    "        trial_timeout=300,\n",
    "    )\n",
    "\n",
    "    hyper_estimator.fit(X_train, y_train)\n",
    "    print(hyper_estimator.best_model())\n",
    "    return hyper_estimator.score(X_test, y_test)\n",
    "\n",
    "apply_hyperopt(df_train, df_test)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
