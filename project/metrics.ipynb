{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d41991-3e59-4587-9904-a43131e20a7d",
   "metadata": {},
   "source": [
    "# TODOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a07b0-8465-4a55-9dc2-ef3a92622f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown('TODO.md'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cefb50",
   "metadata": {},
   "source": [
    "# Design Pattern Recognition with Software Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dd595e-942a-4e01-994d-0c425619a9d8",
   "metadata": {},
   "source": [
    "## Library/Package Imports\n",
    "All required modules should be in the next cell to avoid scattered imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3867f5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore missing imports warnings in vs code\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from typing import Callable\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from typing import Optional, Dict, List\n",
    "import numpy as np\n",
    "from enum import Enum, auto\n",
    "from constants import ClassMetricVectorConstants, get_label_column, get_metric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65910d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common utility functions\n",
    "def generate_subplot(df: pd.DataFrame, plot_func: Callable[[pd.DataFrame, str], go.Figure], subplot_width: int = 600, subplot_height: int = 2400) -> go.Figure:\n",
    "    metric_columns = get_metric_columns()\n",
    "    subplots = make_subplots(\n",
    "        len(metric_columns), subplot_titles=metric_columns)\n",
    "    for i, metric in enumerate(metric_columns):\n",
    "        figure = plot_func(df, metric)\n",
    "        subplots.add_trace(figure, row=i+1, col=1)\n",
    "    subplots['layout'].update(height=subplot_height, width=subplot_width)\n",
    "    return subplots\n",
    "\n",
    "\n",
    "def generate_selectable_graph_for_metrics(df: pd.DataFrame, initial_plot_func: Callable[[], go.Figure], update_func: Callable[[go.Figure, pd.DataFrame, str], None], y_label: Optional[str] = None):\n",
    "    metric_dropdown = widgets.Dropdown(options=get_metric_columns())\n",
    "    fig = go.FigureWidget(initial_plot_func())\n",
    "\n",
    "    def on_metric_changed(change):\n",
    "        metric = change['new']\n",
    "        with fig.batch_update():\n",
    "            figure = fig.data[0]\n",
    "            update_func(figure, df, metric)\n",
    "            figure['name'] = metric\n",
    "            label = y_label if y_label else ' '\n",
    "            fig.update_layout(title=metric, bargap=0.5,\n",
    "                              xaxis_title=metric, yaxis_title=label)\n",
    "\n",
    "    metric_dropdown.observe(on_metric_changed, names='value')\n",
    "    display(widgets.VBox([metric_dropdown, fig]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef9395",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generation of metrics\n",
    "\n",
    "If the metrics are not yet generated, the following steps are required:\n",
    "\n",
    "1. Make sure that `source_files.zip` is located in the current directory. The archive contains the actual zipped source code of the projects in [P-MArT](https://www.ptidej.net/tools/designpatterns/) and `pmart.xml` with descriptions of the micro architectures\n",
    "2. Create a new virtual Python environment with `python -m venv .` in the current directory if not yet done\n",
    "3. Activate the virtual environment ([refer here for the actual command to run](https://docs.python.org/3/library/venv.html#how-venvs-work))\n",
    "4. Execute `python3 preprocess_source_files.py` to extract the source files from `source_files.zip` and move the source files described in `pmart.xml` into `dataset` directory. For more information run `python3 preprocess_source_files.py -h`.\n",
    "    - Source files are structured as `<dataset_dir>/<design_pattern/micro_architecture_<id>`\n",
    "    - Each micro architecture directory contains the following files:\n",
    "        - `roles.csv`: Roles, entity names and role kind as described in `pmart.xml`\n",
    "        - `projects.txt`: From which project the source files come from\n",
    "        - The source files to be evaluated\n",
    "5. \n",
    "    - **OLD**: Execute `python3 generate_source_file_metrics.py` to generate `metrics.csv`. For more information run `python3 generate_source_file_metrics.py`.\n",
    "    - **NEW**: Execute `docker build --file docker/sourcefileparser.dockerfile . -t sourcefilerparser:latest` in the `project` directory to build the tool and run `docker run -v ./:/home/app/volume  -e DATASET_PATH=./dataset -e OUTPUT_CSV=./m.csv sourcefilerparser:latest` for metric generation\n",
    "\n",
    "**NOTES**: \n",
    "- As the projects in this dataset are old and not all projects listed in P-MaRT are not accessible, some source files and their entries in `metrics` may be missing.\n",
    "- The tool for generating the metrics was originally written with a Java Parser implemented Python only. This lead to parsing issues in some source files. As a result, the tool was rewritten as a Java project with a native parser. The original Python script is included for completeness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33566ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Overview about `metrics.csv`\n",
    "\n",
    "In order to detect applied Gang Of Four design patterns in source code with machine learning strategies, we first need to transform the source file into a numerical representation that can be understood by a machine learning model.\n",
    "This approach aims to solve this by generating numerical characteristics for each source file in the context of the regarded micro architecture. As there are several methods to define what metrics to include in the evaluation, the metrics as described [in this paper](../sources/JSEA-DP-2014.pdf):\n",
    "\n",
    "- NOF: Number of fields\n",
    "- NSF: Number of static fields\n",
    "- NOM: Number of methods\n",
    "- NSM: Number of static methods\n",
    "- NOAM: Number of abstract methods\n",
    "- NORM: Number of overridden methods\n",
    "- NOPC: Number of private constrcutors\n",
    "- NOOF: Number of object fields\n",
    "- NCOF: Number of other classes with field of own type\n",
    "\n",
    "\n",
    "In addition to these metrics, the following Chidamber & Kemerer object-oriented metrics were added to quantify the relation, coupling and cohesion between participants in a design pattern:\n",
    "\n",
    "- FAN_IN: Number of input dependencies\n",
    "- FAN_OUT: Number of output dependencies\n",
    "- CBO: Coupling between objects\n",
    "- NOC: Number of inheriting children\n",
    "- RFC: Response for a class (number of unique method invocations in a class)\n",
    "- TCC: Tight class cohesion (via direct connections between visible methods, two methods or their invocation trees access the same class variable)\n",
    "- LCC: Low class cohesion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ab6a56-c832-4e2d-93d5-ad5ea97e5bf8",
   "metadata": {},
   "source": [
    "## Outlier Detection and Removal\n",
    "\n",
    "As the dataset may contain a varied implementation of datasets, outlier detection and removal may be required to reduce the noise in the dataset. `sklearn` provides the some automatic and unsupervised approaches out of the box. The following are considered\n",
    "\n",
    "**NOTE**: This list is subject to change\n",
    "\n",
    "* Isolation Forest\n",
    "* Local Outlier Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a743b-1abd-41fc-8d74-837e10a5e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports for this section\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a025d56",
   "metadata": {},
   "source": [
    "### Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1daa46b-d8c0-4008-95ac-97820f595da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_isolation_forest(df: pd.DataFrame):\n",
    "    df_filtered = df.copy()\n",
    "    isolation_forest = IsolationForest(contamination=0.1)\n",
    "    df_filtered['outlier'] = isolation_forest.fit_predict(\n",
    "        df_filtered[get_metric_columns()])\n",
    "    df_filtered = df_filtered[df_filtered['outlier'] == 1]\n",
    "    return df_filtered.drop(columns=['outlier'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d4fcca",
   "metadata": {},
   "source": [
    "### Local Outlier Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34133435-52f9-42a7-824f-9fe1aaf96183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_local_outlier_factor(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    threshold = 0\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    lof = LocalOutlierFactor(contamination=0.5)\n",
    "    df_copy['outlier_score'] = lof.fit_predict(df_copy[get_metric_columns()])\n",
    "    df_copy = df_copy[df_copy['outlier_score'] > threshold]\n",
    "    return df_copy.drop(columns=('outlier_score'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f390ae",
   "metadata": {},
   "source": [
    "## Explorative Data Analysis of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c7b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./metrics.csv')\n",
    "df = df.dropna()\n",
    "#df = apply_isolation_forest(df)\n",
    "print(f'{df.shape[0]} rows were imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d63a37-a043-43ba-aca0-693e0e6ccabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[ClassMetricVectorConstants.ROLE] = df[ClassMetricVectorConstants.ROLE].str.lower().str.strip()\n",
    "df[ClassMetricVectorConstants.ROLE_KIND] = df[ClassMetricVectorConstants.ROLE_KIND].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9d1680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if columns in dataframe have expected types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f534e36",
   "metadata": {},
   "source": [
    "### Filter Dataframe entries by micro architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24e61d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_arches = df[ClassMetricVectorConstants.MICRO_ARCHITECTURE].unique().tolist()\n",
    "\n",
    "def view(micro_arch=''):\n",
    "    cols = [ClassMetricVectorConstants.ROLE_KIND, ClassMetricVectorConstants.ENTITY] + get_metric_columns()\n",
    "    display(df[df[ClassMetricVectorConstants.MICRO_ARCHITECTURE] == micro_arch]\n",
    "            [cols], clear=True)\n",
    "\n",
    "\n",
    "w = widgets.Dropdown(options=micro_arches)\n",
    "widgets.interactive(view, micro_arch=w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0616d25e-d4f0-4d42-91a2-6e33586a80e7",
   "metadata": {},
   "source": [
    "### Corelation Between Columns\n",
    "For each column we caclulate pairwaise the coefficient of corelation with other columns. The value of the coefficient can be interpreteted as:\n",
    "\n",
    "- between -1.0 and 0: Negative correlation; a increase in one column expects a decrease in the other; the lower the bigger the impact\n",
    "- equals 0: No correlation\n",
    "- between 0 and 1: Postive correlation; a increase in one column causes an increase the other; the higher the bigger the impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab51ef0-64b7-4858-ae32-20a1be678bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df[get_metric_columns()].copy()\n",
    "corr = df_corr.corr()\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        x=corr.columns,\n",
    "        y=corr.index,\n",
    "        z=np.array(corr),\n",
    "        text=corr.values,\n",
    "        texttemplate='%{text:.2f}'\n",
    "    )\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b016f497",
   "metadata": {},
   "source": [
    "### Distribution of roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79959eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = df.groupby([ClassMetricVectorConstants.ROLE]).size()\n",
    "temp = temp.sort_values(ascending=False).reset_index()\n",
    "px.bar(temp, x=ClassMetricVectorConstants.ROLE, y=0).update_layout(yaxis_title='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240b2766-9c37-4c29-a2d8-cee945d1ba17",
   "metadata": {},
   "source": [
    "### Distribution of design patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f654cb14-24d4-403d-ad90-7db5bbf0e1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binned_by_role = df.copy()\n",
    "df_binned_by_role = df_binned_by_role.drop_duplicates(\n",
    "    [ClassMetricVectorConstants.MICRO_ARCHITECTURE, ClassMetricVectorConstants.DESIGN_PATTERN]).reset_index()\n",
    "df_binned_by_role = df_binned_by_role[ClassMetricVectorConstants.DESIGN_PATTERN].value_counts(\n",
    ").reset_index()\n",
    "\n",
    "fig = px.histogram(df_binned_by_role, x=ClassMetricVectorConstants.DESIGN_PATTERN, y='count')\n",
    "fig.update_layout(xaxis_title='Design Pattern',\n",
    "                  yaxis_title='Count of Design Pattern')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca83e997",
   "metadata": {},
   "source": [
    "### Distribution for metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236b326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_histogram():\n",
    "    return go.Histogram(\n",
    "        histfunc='count',\n",
    "    )\n",
    "\n",
    "\n",
    "def update_histogram(figure: go.Figure, df: pd.DataFrame, metric: str):\n",
    "    figure['x'] = df[metric]\n",
    "\n",
    "\n",
    "generate_selectable_graph_for_metrics(\n",
    "    df, initial_histogram, update_histogram, 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e5650",
   "metadata": {},
   "source": [
    "### Box Plots for metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51e3449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_histogram():\n",
    "    return go.Box(\n",
    "    )\n",
    "\n",
    "\n",
    "def update_histogram(figure: go.Figure, df: pd.DataFrame, metric: str):\n",
    "    figure['x'] = df[metric]\n",
    "\n",
    "\n",
    "generate_selectable_graph_for_metrics(df, initial_histogram, update_histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa3c25",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "As design patterns can be considered as small scale appliances of software architecture, they consist of different entities with different relationships and roles to fulfill in the regarded design pattern. In order to detect design patterns, we first need to detect what kind of role a given Java class or entity it most likely corresponds to. To achieve this, machine learning model capable of classifying multiple labels should be considered. The extracted software metrics are the numerical inputs and the most likely roles in a design pattern are the result. \n",
    "As this falls in the area of supervised machine learning, initially the following models/techniques are to be considered:\n",
    "\n",
    "**NOTE:** This list is subject to change \n",
    "\n",
    "* Support Vector Machines\n",
    "* Tree Classifiers\n",
    "* Ensemble Classifiers (e.g Random Forest Classifier)\n",
    "* Custom Convoluted Network\n",
    "\n",
    "In order to optimize the given results of a given model, first RandomGridSearch is applied to determine a range of values or selection for the hyperparameters while GridSearch is used to determine the most optimal available value or selection for the regarded hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "edebf2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required import for machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import hpsklearn\n",
    "import hyperopt\n",
    "from dataclasses import dataclass, field\n",
    "import joblib\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import numpy as np\n",
    "from sklearn.base import clone\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from collections import defaultdict\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065cf01b-0f73-41ad-bab3-fdffdd3775e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    train: pd.DataFrame\n",
    "    test: pd.DataFrame\n",
    "    label_col: List[str]\n",
    "    feature_cols: List[str]\n",
    "    #roleKindEncoder: LabelEncoder\n",
    "    roleEncoder: LabelEncoder\n",
    "    dataset: pd.DataFrame\n",
    "    \n",
    "    def __init__(self, train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
    "        self.label_col = get_label_column()\n",
    "        self.feature_cols = get_metric_columns()\n",
    "        self.train = train_df\n",
    "        self.test = test_df\n",
    "        self.roleEncoder = LabelEncoder()\n",
    "        self.roleEncoder.fit(self.train[ClassMetricVectorConstants.ROLE])\n",
    "        train_df[ClassMetricVectorConstants.ROLE] = self.roleEncoder.transform(train_df[ClassMetricVectorConstants.ROLE])\n",
    "        test_df[ClassMetricVectorConstants.ROLE] = self.roleEncoder.transform(test_df[ClassMetricVectorConstants.ROLE])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def top_k_design_patterns(cls, df: pd.DataFrame, k: int) -> \"Dataset\":\n",
    "        train, test = split_by_micro_arch(df, k)\n",
    "        return cls(train, test)\n",
    "        \n",
    "\n",
    "    def get_X_train(self):\n",
    "        return self.train[self.feature_cols]\n",
    "    \n",
    "    def get_Y_train(self):\n",
    "        return self.train[self.label_col].values.ravel()\n",
    "\n",
    "    def get_X_test(self):\n",
    "        return self.test[self.feature_cols]\n",
    "\n",
    "    def get_Y_test(self):\n",
    "        return self.test[self.label_col].values.ravel()\n",
    "\n",
    "\n",
    "def get_top_k_labels(df: pd.DataFrame, k: int):\n",
    "    df_binned_by_role = df.copy()\n",
    "    df_binned_by_role = df_binned_by_role.drop_duplicates(\n",
    "        [ClassMetricVectorConstants.MICRO_ARCHITECTURE, ClassMetricVectorConstants.DESIGN_PATTERN])\n",
    "    df_binned_by_role = df_binned_by_role[ClassMetricVectorConstants.DESIGN_PATTERN].value_counts(\n",
    "    ).sort_values(ascending=False).head(k)\n",
    "    return df_binned_by_role.index.to_list()\n",
    "\n",
    "\n",
    "def split_by_micro_arch(df: pd.DataFrame, k: int):\n",
    "    train_data_size = 0.4\n",
    "    dp_map = defaultdict(list)\n",
    "    top_patterns = get_top_k_labels(df, k)\n",
    "    df_top_k = df[df[ClassMetricVectorConstants.DESIGN_PATTERN].isin(top_patterns)].reset_index()\n",
    "    micro_arches = df_top_k[ClassMetricVectorConstants.MICRO_ARCHITECTURE].unique().tolist()\n",
    "    for m in micro_arches:\n",
    "        micro_rows = df_top_k[df_top_k[ClassMetricVectorConstants.MICRO_ARCHITECTURE] == m].copy().reset_index(drop=True)\n",
    "        dp_key = micro_rows[ClassMetricVectorConstants.DESIGN_PATTERN].iloc[0]\n",
    "        dp_map[dp_key].append(micro_rows)\n",
    "    [dfs.sort(key=lambda l: -len(l)) for dfs in dp_map.values()]\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    for micro_arches in dp_map.values():\n",
    "        list_size = len(micro_arches)\n",
    "        train_size = round(list_size * train_data_size)\n",
    "\n",
    "        train_data.append(pd.concat(micro_arches[:train_size].copy()).reset_index(drop=True))\n",
    "        test_data.append(pd.concat(micro_arches[train_size:].copy()).reset_index(drop=True))\n",
    "\n",
    "    train = pd.concat(train_data, ignore_index=True)\n",
    "    test = pd.concat(test_data, ignore_index=True)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def scoring(target, pred):\n",
    "        return -f1_score(target, pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdad0cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.top_k_design_patterns(df, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7df3789-9e1f-4ae3-8efd-6606839eaad0",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006c8ba1-fb69-4239-8f78-835b8a383c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_svm(dataset: Dataset):\n",
    "    X_train = dataset.get_X_train()\n",
    "    y_train = dataset.get_Y_train()\n",
    "    \n",
    "    X_test = dataset.get_X_test()\n",
    "    y_test = dataset.get_Y_test()\n",
    "\n",
    "    standard_scaler = StandardScaler()\n",
    "    X_train = standard_scaler.fit_transform(X_train)\n",
    "    X_test = standard_scaler.fit_transform(X_test)\n",
    "\n",
    "    svm_classifier = SVC(kernel='rbf', gamma=0.1, C=1.75)\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "    pred = svm_classifier.predict(X_test)\n",
    "    return f1_score(y_test, pred, average='micro')\n",
    "\n",
    "apply_svm(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0516a563-d27b-401d-ae1c-16bc4528c458",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395b288b-1501-4262-9a99-2711a575581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_random_forest(dataset: Dataset):\n",
    "    X_train = dataset.get_X_train()\n",
    "    y_train = dataset.get_Y_train()\n",
    "\n",
    "    X_test = dataset.get_X_test()\n",
    "    y_test = dataset.get_Y_test()\n",
    "\n",
    "    standard_scaler = StandardScaler()\n",
    "    X_train = standard_scaler.fit_transform(X_train)\n",
    "    X_test = standard_scaler.fit_transform(X_test)\n",
    "\n",
    "    random_forest_classifier = RandomForestClassifier(\n",
    "        max_depth=30, random_state=1)\n",
    "    random_forest_classifier.fit(X_train, y_train)\n",
    "\n",
    "    pred = random_forest_classifier.predict(X_test)\n",
    "    return  f1_score(y_test, pred, average='micro')\n",
    "\n",
    "\n",
    "apply_random_forest(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aa5f3c-7044-4840-9c10-b6a11691a5f0",
   "metadata": {},
   "source": [
    "### Get Best Possible Classifier with hyperopt-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c407fe-985c-48c2-bbb7-8e5fe37f959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_hyperopt(dataset: Dataset, evals: int = 10):\n",
    "    \n",
    "    \n",
    "    X_train = dataset.get_X_train()\n",
    "    y_train = dataset.get_Y_train()\n",
    "\n",
    "\n",
    "    X_test = dataset.get_X_test()\n",
    "    y_test = dataset.get_Y_test()\n",
    "\n",
    "\n",
    "    standard_scaler = StandardScaler()\n",
    "    X_train = standard_scaler.fit_transform(X_train)\n",
    "    X_test = standard_scaler.fit_transform(X_test)\n",
    "    \n",
    "    chosen_classifiers = [\n",
    "        hpsklearn.random_forest_classifier('random_forest'),\n",
    "        hpsklearn.k_neighbors_classifier('knn'),\n",
    "        hpsklearn.svc('svm')\n",
    "    ]\n",
    "\n",
    "    p = 1 / len(chosen_classifiers)\n",
    "    classifiers = hyperopt.hp.pchoice('cls', [(p, c) for c in chosen_classifiers])\n",
    "\n",
    "    hyper_estimator = hpsklearn.HyperoptEstimator(\n",
    "        classifier=classifiers,\n",
    "        preprocessing=[],\n",
    "        max_evals=evals,\n",
    "        algo=hyperopt.tpe.suggest,\n",
    "        trial_timeout=20,\n",
    "        loss_fn=scoring,\n",
    "        \n",
    "        \n",
    "    )\n",
    "\n",
    "    hyper_estimator.fit(X_train, y_train)\n",
    "    best_model = hyper_estimator.best_model()['learner']\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return hyper_estimator.score(X_test, y_test), best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "470b4886-38fc-4331-bb96-a7dcb12c2986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.05trial/s, best loss: -0.0]\n",
      "100%|██████████| 2/2 [00:00<00:00,  1.02trial/s, best loss: -0.4]\n",
      "100%|██████████| 3/3 [00:00<00:00,  2.07trial/s, best loss: -0.4]\n",
      "100%|██████████| 4/4 [00:00<00:00,  1.02trial/s, best loss: -0.4]\n",
      "100%|██████████| 5/5 [00:00<00:00,  2.06trial/s, best loss: -0.4]\n",
      "100%|██████████| 6/6 [00:00<00:00,  2.10trial/s, best loss: -0.4]\n",
      "100%|██████████| 7/7 [00:00<00:00,  2.10trial/s, best loss: -0.4]\n",
      "100%|██████████| 8/8 [00:00<00:00,  2.09trial/s, best loss: -0.4]\n",
      "100%|██████████| 9/9 [00:00<00:00,  1.02trial/s, best loss: -0.4]\n",
      "100%|██████████| 10/10 [00:00<00:00,  2.06trial/s, best loss: -0.4]\n",
      "100%|██████████| 11/11 [00:00<00:00,  2.06trial/s, best loss: -0.4]\n",
      "100%|██████████| 12/12 [00:04<00:00,  4.01s/trial, best loss: -0.4]\n",
      "100%|██████████| 13/13 [00:00<00:00,  2.04trial/s, best loss: -0.4]\n",
      "100%|██████████| 14/14 [00:00<00:00,  2.07trial/s, best loss: -0.4]\n",
      "100%|██████████| 15/15 [00:00<00:00,  1.02trial/s, best loss: -0.4]\n",
      "100%|██████████| 16/16 [00:00<00:00,  2.03trial/s, best loss: -0.4]\n",
      "100%|██████████| 17/17 [00:00<00:00,  1.02trial/s, best loss: -0.4]\n",
      "100%|██████████| 18/18 [00:00<00:00,  2.09trial/s, best loss: -0.4]\n",
      "100%|██████████| 19/19 [00:00<00:00,  1.02trial/s, best loss: -0.4]\n",
      "100%|██████████| 20/20 [00:00<00:00,  2.05trial/s, best loss: -0.4]\n",
      "100%|██████████| 21/21 [00:00<00:00,  2.03trial/s, best loss: -0.4]\n",
      "100%|██████████| 22/22 [00:00<00:00,  1.99trial/s, best loss: -0.4]\n",
      "100%|██████████| 23/23 [00:01<00:00,  1.29s/trial, best loss: -0.4]\n",
      "100%|██████████| 24/24 [00:01<00:00,  1.24s/trial, best loss: -0.4]\n",
      "100%|██████████| 25/25 [00:00<00:00,  2.00trial/s, best loss: -0.4]\n",
      "100%|██████████| 26/26 [00:00<00:00,  1.25trial/s, best loss: -0.4]\n",
      "100%|██████████| 27/27 [00:00<00:00,  2.02trial/s, best loss: -0.4]\n",
      "100%|██████████| 28/28 [00:00<00:00,  2.01trial/s, best loss: -0.4]\n",
      "100%|██████████| 29/29 [00:00<00:00,  2.03trial/s, best loss: -0.4]\n",
      " 97%|█████████▋| 29/30 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-124:\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "\n",
      "  File \"/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/hpsklearn/estimator/_cost_fn.py\", line 211, in _cost_fn\n",
      "    learner.fit(XEXfit, yfit)\n",
      "\n",
      "  File \"/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "\n",
      "  File \"/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\", line 478, in fit\n",
      "    trees = [\n",
      "\n",
      "  File \"/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\", line 479, in <listcomp>\n",
      "    self._make_estimator(append=False, random_state=random_state)\n",
      "\n",
      "  File \"/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/ensemble/_base.py\", line 145, in _make_estimator\n",
      "    _set_random_states(estimator, random_state)\n",
      "\n",
      "  File \"/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/ensemble/_base.py\", line 70, in _set_random_states\n",
      "    for key in sorted(estimator.get_params(deep=True)):\n",
      "\n",
      "  File \"/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/base.py\", line 243, in get_params\n",
      "    for key in self._get_param_names():\n",
      "\n",
      "  File \"/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/base.py\", line 210, in _get_param_names\n",
      "    parameters = [\n",
      "\n",
      "  File \"/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/base.py\", line 213, in <listcomp>\n",
      "    if p.name != \"self\" and p.kind != p.VAR_KEYWORD\n",
      "\n",
      "  File \"/usr/lib/python3.10/inspect.py\", line 2687, in name\n",
      "    @property\n",
      "\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 1457, in _pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\n",
      "\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 1758, in _pydevd_bundle.pydevd_cython.ThreadTracer.__call__\n",
      "\n",
      "  File \"/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_is_thread_alive.py\", line 9, in is_thread_alive\n",
      "    def is_thread_alive(t):\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.00      0.00      0.00         3\n",
      "           2       0.00      0.00      0.00        11\n",
      "           3       1.00      0.25      0.40         4\n",
      "           4       0.00      0.00      0.00        14\n",
      "           5       0.00      0.00      0.00        12\n",
      "           6       0.57      0.33      0.42        12\n",
      "           7       0.26      0.75      0.39         8\n",
      "           8       0.25      0.12      0.17         8\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       0.33      0.11      0.17         9\n",
      "          11       0.00      0.00      0.00         3\n",
      "          12       0.03      0.33      0.06         3\n",
      "\n",
      "    accuracy                           0.15        95\n",
      "   macro avg       0.19      0.15      0.12        95\n",
      "weighted avg       0.19      0.15      0.13        95\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'HyperOpt-Score: 0.14736842105263157 Mean Cross Validation Score: [0.17592593 0.21223545 0.29604236 0.24490741 0.21132713]'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_best_iteration(df: pd.DataFrame, top_k: int, max_evals: int, k_split: int):\n",
    "    dataset = Dataset.top_k_design_patterns(df, top_k)\n",
    "    score, estimator = apply_hyperopt(dataset, evals=max_evals)\n",
    "    #unfitted_estimator = clone(estimator)\n",
    "    cross_score = cross_val_score(estimator, dataset.get_X_train(), dataset.get_Y_train(), cv=k_split, scoring='f1_macro')\n",
    "    joblib.dump(estimator, 'estimater.joblib')\n",
    "    return f'HyperOpt-Score: {score} Mean Cross Validation Score: {cross_score}'\n",
    "    \n",
    "get_best_iteration(df, 4, 50, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1567d962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adaptee & 0.02 & 0.33 & 0.04\\\\adapter & 0.00 & 0.00 & 0.00\\\\client & 0.00 & 0.00 & 0.00\\\\command & 0.00 & 0.00 & 0.00\\\\concreteCommand & 0.43 & 0.93 & 0.59\\\\concreteObserver & 0.00 & 0.00 & 0.00\\\\concreteSubject & 0.00 & 0.00 & 0.00\\\\invoker & 0.10 & 0.25 & 0.14\\\\observer & 0.00 & 0.00 & 0.00\\\\receiver & 0.00 & 0.00 & 0.00\\\\singleton & 0.00 & 0.00 & 0.00\\\\subject & 0.00 & 0.00 & 0.00\\\\target & 0.00 & 0.00 & 0.00\\\\accuracy & 0.17 & 95\\\\macro & avg & 0.04 & 0.12\\\\weighted & avg & 0.07 & 0.17\\\\\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "s = \"adaptee       0.02      0.33      0.04         3\\n         adapter       0.00      0.00      0.00         3\\n          client       0.00      0.00      0.00        11\\n         command       0.00      0.00      0.00         4\\n concreteCommand       0.43      0.93      0.59        14\\nconcreteObserver       0.00      0.00      0.00        12\\n concreteSubject       0.00      0.00      0.00        12\\n         invoker       0.10      0.25      0.14         8\\n        observer       0.00      0.00      0.00         8\\n        receiver       0.00      0.00      0.00         5\\n       singleton       0.00      0.00      0.00         9\\n         subject       0.00      0.00      0.00         3\\n          target       0.00      0.00      0.00         3\\n\\n        accuracy                           0.17        95\\n       macro avg       0.04      0.12      0.06        95\\n    weighted avg       0.07      0.17      0.10        95\\n\"\n",
    "tab = \"\"\n",
    "for r in s.split(\"\\n\"):\n",
    "    t = [i for i in re.split(r'\\s', r) if i.strip()]\n",
    "    t = t[:4]\n",
    "    if t:\n",
    "        tab= tab + \" & \".join(t) + \"\\\\\\\\\"\n",
    "\n",
    "print(tab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6f516cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adapter', 'Adapter', 'Adapter', 'Adapter', 'Command', 'Command', 'Command', 'Command', 'Observer', 'Observer', 'Observer', 'Observer', 'Singleton', 'Singleton', 'Singleton', 'Singleton']\n",
      "['Adapter', 'Command', 'Adapter', 'Command', 'Observer', 'Observer', 'Singleton', 'Command', 'Observer', 'Adapter', 'Observer', 'Command', 'Command', 'Command', 'Command', 'Command']\n",
      "Entire Process\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Adapter       0.67      0.50      0.57         4\n",
      "     Command       0.12      0.25      0.17         4\n",
      "    Observer       0.50      0.50      0.50         4\n",
      "   Singleton       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.31        16\n",
      "   macro avg       0.32      0.31      0.31        16\n",
      "weighted avg       0.32      0.31      0.31        16\n",
      "\n",
      "Model\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "         adaptee       0.02      0.33      0.04         3\n",
      "         adapter       0.00      0.00      0.00         3\n",
      "          client       0.00      0.00      0.00        11\n",
      "         command       0.00      0.00      0.00         4\n",
      " concreteCommand       0.43      0.93      0.59        14\n",
      "concreteObserver       0.00      0.00      0.00        12\n",
      " concreteSubject       0.00      0.00      0.00        12\n",
      "         invoker       0.10      0.25      0.14         8\n",
      "        observer       0.00      0.00      0.00         8\n",
      "        receiver       0.00      0.00      0.00         5\n",
      "       singleton       0.00      0.00      0.00         9\n",
      "         subject       0.00      0.00      0.00         3\n",
      "          target       0.00      0.00      0.00         3\n",
      "\n",
      "        accuracy                           0.17        95\n",
      "       macro avg       0.04      0.12      0.06        95\n",
      "    weighted avg       0.07      0.17      0.10        95\n",
      "\n",
      "Params for Random Forest\n",
      "{'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': None, 'max_features': 0.838038061415148, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 5, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 161, 'n_jobs': 1, 'oob_score': False, 'random_state': 0, 'verbose': False, 'warm_start': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class DpRole:\n",
    "    role_name: str\n",
    "    mutliple_occurences: bool = field(default=False)\n",
    "\n",
    "#Singelton-, Adapter-, Command- und Observer-Entwurfsmuster \n",
    "design_patterns_to_detect = {\n",
    "    'Singleton': [DpRole('singleton')],\n",
    "    'Adapter': [DpRole('target'), DpRole('client', True), DpRole('adaptee', True), DpRole('adapter', True)],\n",
    "    'Command': [DpRole('command'), DpRole('concreteCommand', True), DpRole('client'), DpRole('invoker'), DpRole('receiver')],\n",
    "    'Observer': [DpRole('subject'), DpRole('observer'), DpRole('concreteSubject'), DpRole('concreteObserver', True)]\n",
    "}\n",
    "\n",
    "\n",
    "dataset = Dataset.top_k_design_patterns(df, 4)\n",
    "random_micro_arch = dataset.test.groupby([ClassMetricVectorConstants.DESIGN_PATTERN]).sample(4)[ClassMetricVectorConstants.MICRO_ARCHITECTURE].to_list()\n",
    "\n",
    "vectors = []\n",
    "for m in random_micro_arch:\n",
    "    v = dataset.test[dataset.test[ClassMetricVectorConstants.MICRO_ARCHITECTURE] == m].copy().reset_index()\n",
    "    vectors.append(v)\n",
    "\n",
    "model = joblib.load('estimater.joblib')\n",
    "\n",
    "def match_pattern(role_encoder: LabelEncoder, roles_df: pd.DataFrame, dps: dict):\n",
    "    # X = roles_df[get_metric_columns()]\n",
    "    # y = model.predict(X)\n",
    "    # roles = role_encoder.inverse_transform(y.ravel())\n",
    "    # scores = defaultdict(int)\n",
    "    # for dp, dp_roles in dps.items():\n",
    "    #     single_roles = set()\n",
    "    #     role_freq = defaultdict(int)\n",
    "    #     for role in roles:\n",
    "    #         dp_role = None\n",
    "    #         for d in dp_roles:\n",
    "    #             if d.role_name == role:\n",
    "    #                 dp_role = d\n",
    "    #                 break\n",
    "    #         if dp_role and not dp_role.mutliple_occurences and dp_role.role_name not in single_roles:\n",
    "    #             role_freq[dp_role.role_name] += 1\n",
    "    #             single_roles.add(dp_role.role_name)\n",
    "    #         elif dp_role and dp_role.role_name not in single_roles and dp_role.mutliple_occurences:\n",
    "    #             role_freq[dp_role.role_name] += 1\n",
    "    #         else:\n",
    "    #             role_freq[role] = 0\n",
    "    #     score = sum([freq for freq in role_freq.values()])/len(roles)\n",
    "    #     scores[dp] = score\n",
    "    # return scores\n",
    "\n",
    "    X = roles_df[get_metric_columns()]\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    y = model.predict(X)\n",
    "    roles = role_encoder.inverse_transform(y.ravel())\n",
    "    scores = defaultdict(float)\n",
    "\n",
    "    predicted_role_freq = defaultdict(int)\n",
    "    for role in roles:\n",
    "        predicted_role_freq[role] += 1\n",
    "\n",
    "    for dp, dp_roles in dps.items():\n",
    "        matched_roles = defaultdict(int)\n",
    "\n",
    "        dp_roles_dict = {d.role_name: d for d in dp_roles}\n",
    "\n",
    "        for role in predicted_role_freq:\n",
    "            if role in dp_roles_dict:\n",
    "                dp_role = dp_roles_dict[role]\n",
    "                if (dp_role.mutliple_occurences or predicted_role_freq[role] == 1) and matched_roles[role] < predicted_role_freq[role]:\n",
    "                    matched_roles[role] += 1\n",
    "\n",
    "        total_possible_matches = sum([1 for d in dp_roles if d.mutliple_occurences or d.role_name in predicted_role_freq])\n",
    "        score = sum(matched_roles.values()) / total_possible_matches if total_possible_matches > 0 else 0\n",
    "        scores[dp] = score\n",
    "\n",
    "    return scores\n",
    "\n",
    "labels = []\n",
    "preds = []\n",
    "for v in vectors:\n",
    "    label = v[ClassMetricVectorConstants.DESIGN_PATTERN][0]\n",
    "    labels.append(label)\n",
    "    predictions = match_pattern(dataset.roleEncoder, v, design_patterns_to_detect)\n",
    "    p = max(predictions.items(), key=lambda x: x[1])\n",
    "    preds.append(p[0])\n",
    "\n",
    "print(labels)\n",
    "print(preds)\n",
    "print('Entire Process')\n",
    "print(classification_report(labels, preds))\n",
    "\n",
    "print('Model')\n",
    "predictions = model.predict(dataset.get_X_test())\n",
    "predictions = dataset.roleEncoder.inverse_transform(predictions)\n",
    "real_labels = dataset.roleEncoder.inverse_transform(dataset.get_Y_test())\n",
    "print(classification_report(real_labels, predictions))\n",
    "\n",
    "    \n",
    "\n",
    "print(\"Params for Random Forest\")\n",
    "print(model.get_params())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
