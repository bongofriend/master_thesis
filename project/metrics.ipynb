{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d41991-3e59-4587-9904-a43131e20a7d",
   "metadata": {},
   "source": [
    "# TODOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a07b0-8465-4a55-9dc2-ef3a92622f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown('TODO.md'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cefb50",
   "metadata": {},
   "source": [
    "# Design Pattern Recognition with Software Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dd595e-942a-4e01-994d-0c425619a9d8",
   "metadata": {},
   "source": [
    "## Library/Package Imports\n",
    "All required modules should be in the next cell to avoid scattered imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3867f5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore missing imports warnings in vs code\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from typing import Callable\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from typing import Optional, Dict, List\n",
    "import numpy as np\n",
    "from enum import Enum, auto\n",
    "from constants import ClassMetricVectorConstants, get_label_column, get_metric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65910d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common utility functions\n",
    "def generate_subplot(df: pd.DataFrame, plot_func: Callable[[pd.DataFrame, str], go.Figure], subplot_width: int = 600, subplot_height: int = 2400) -> go.Figure:\n",
    "    metric_columns = get_metric_columns()\n",
    "    subplots = make_subplots(\n",
    "        len(metric_columns), subplot_titles=metric_columns)\n",
    "    for i, metric in enumerate(metric_columns):\n",
    "        figure = plot_func(df, metric)\n",
    "        subplots.add_trace(figure, row=i+1, col=1)\n",
    "    subplots['layout'].update(height=subplot_height, width=subplot_width)\n",
    "    return subplots\n",
    "\n",
    "\n",
    "def generate_selectable_graph_for_metrics(df: pd.DataFrame, initial_plot_func: Callable[[], go.Figure], update_func: Callable[[go.Figure, pd.DataFrame, str], None], y_label: Optional[str] = None):\n",
    "    metric_dropdown = widgets.Dropdown(options=get_metric_columns())\n",
    "    fig = go.FigureWidget(initial_plot_func())\n",
    "\n",
    "    def on_metric_changed(change):\n",
    "        metric = change['new']\n",
    "        with fig.batch_update():\n",
    "            figure = fig.data[0]\n",
    "            update_func(figure, df, metric)\n",
    "            figure['name'] = metric\n",
    "            label = y_label if y_label else ' '\n",
    "            fig.update_layout(title=metric, bargap=0.5,\n",
    "                              xaxis_title=metric, yaxis_title=label)\n",
    "\n",
    "    metric_dropdown.observe(on_metric_changed, names='value')\n",
    "    display(widgets.VBox([metric_dropdown, fig]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef9395",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generation of metrics\n",
    "\n",
    "If the metrics are not yet generated, the following steps are required:\n",
    "\n",
    "1. Make sure that `source_files.zip` is located in the current directory. The archive contains the actual zipped source code of the projects in [P-MArT](https://www.ptidej.net/tools/designpatterns/) and `pmart.xml` with descriptions of the micro architectures\n",
    "2. Create a new virtual Python environment with `python -m venv .` in the current directory if not yet done\n",
    "3. Activate the virtual environment ([refer here for the actual command to run](https://docs.python.org/3/library/venv.html#how-venvs-work))\n",
    "4. Execute `python3 preprocess_source_files.py` to extract the source files from `source_files.zip` and move the source files described in `pmart.xml` into `dataset` directory. For more information run `python3 preprocess_source_files.py -h`.\n",
    "    - Source files are structured as `<dataset_dir>/<design_pattern/micro_architecture_<id>`\n",
    "    - Each micro architecture directory contains the following files:\n",
    "        - `roles.csv`: Roles, entity names and role kind as described in `pmart.xml`\n",
    "        - `projects.txt`: From which project the source files come from\n",
    "        - The source files to be evaluated\n",
    "5. \n",
    "    - **OLD**: Execute `python3 generate_source_file_metrics.py` to generate `metrics.csv`. For more information run `python3 generate_source_file_metrics.py`.\n",
    "    - **NEW**: Execute `docker build --file docker/sourcefileparser.dockerfile . -t sourcefilerparser:latest` in the `project` directory to build the tool and run `docker run -v ./:/home/app/volume  -e DATASET_PATH=./dataset -e OUTPUT_CSV=./m.csv sourcefilerparser:latest` for metric generation\n",
    "\n",
    "**NOTES**: \n",
    "- As the projects in this dataset are old and not all projects listed in P-MaRT are not accessible, some source files and their entries in `metrics` may be missing.\n",
    "- The tool for generating the metrics was originally written with a Java Parser implemented Python only. This lead to parsing issues in some source files. As a result, the tool was rewritten as a Java project with a native parser. The original Python script is included for completeness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33566ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Overview about `metrics.csv`\n",
    "\n",
    "In order to detect applied Gang Of Four design patterns in source code with machine learning strategies, we first need to transform the source file into a numerical representation that can be understood by a machine learning model.\n",
    "This approach aims to solve this by generating numerical characteristics for each source file in the context of the regarded micro architecture. As there are several methods to define what metrics to include in the evaluation, the metrics as described [in this paper](../sources/JSEA-DP-2014.pdf):\n",
    "\n",
    "- NOF: Number of fields\n",
    "- NSF: Number of static fields\n",
    "- NOM: Number of methods\n",
    "- NSM: Number of static methods\n",
    "- NOAM: Number of abstract methods\n",
    "- NORM: Number of overridden methods\n",
    "- NOPC: Number of private constrcutors\n",
    "- NOOF: Number of object fields\n",
    "- NCOF: Number of other classes with field of own type\n",
    "\n",
    "\n",
    "In addition to these metrics, the following Chidamber & Kemerer object-oriented metrics were added to quantify the relation, coupling and cohesion between participants in a design pattern:\n",
    "\n",
    "- FAN_IN: Number of input dependencies\n",
    "- FAN_OUT: Number of output dependencies\n",
    "- CBO: Coupling between objects\n",
    "- NOC: Number of inheriting children\n",
    "- RFC: Response for a class (number of unique method invocations in a class)\n",
    "- TCC: Tight class cohesion (via direct connections between visible methods, two methods or their invocation trees access the same class variable)\n",
    "- LCC: Low class cohesion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ab6a56-c832-4e2d-93d5-ad5ea97e5bf8",
   "metadata": {},
   "source": [
    "## Outlier Detection and Removal\n",
    "\n",
    "As the dataset may contain a varied implementation of datasets, outlier detection and removal may be required to reduce the noise in the dataset. `sklearn` provides the some automatic and unsupervised approaches out of the box. The following are considered\n",
    "\n",
    "**NOTE**: This list is subject to change\n",
    "\n",
    "* Isolation Forest\n",
    "* Local Outlier Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a743b-1abd-41fc-8d74-837e10a5e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports for this section\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a025d56",
   "metadata": {},
   "source": [
    "### Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1daa46b-d8c0-4008-95ac-97820f595da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_isolation_forest(df: pd.DataFrame):\n",
    "    df_filtered = df.copy()\n",
    "    isolation_forest = IsolationForest(contamination=0.1)\n",
    "    df_filtered['outlier'] = isolation_forest.fit_predict(\n",
    "        df_filtered[get_metric_columns()])\n",
    "    df_filtered = df_filtered[df_filtered['outlier'] == 1]\n",
    "    return df_filtered.drop(columns=['outlier'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d4fcca",
   "metadata": {},
   "source": [
    "### Local Outlier Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34133435-52f9-42a7-824f-9fe1aaf96183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_local_outlier_factor(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    threshold = 0\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    lof = LocalOutlierFactor(contamination=0.5)\n",
    "    df_copy['outlier_score'] = lof.fit_predict(df_copy[get_metric_columns()])\n",
    "    df_copy = df_copy[df_copy['outlier_score'] > threshold]\n",
    "    return df_copy.drop(columns=('outlier_score'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f390ae",
   "metadata": {},
   "source": [
    "## Explorative Data Analysis of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "65c7b79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1060 rows were imported\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./metrics.csv')\n",
    "df = df.dropna()\n",
    "#df = apply_isolation_forest(df)\n",
    "print(f'{df.shape[0]} rows were imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4d63a37-a043-43ba-aca0-693e0e6ccabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[ClassMetricVectorConstants.ROLE] = df[ClassMetricVectorConstants.ROLE].str.lower().str.strip()\n",
    "df[ClassMetricVectorConstants.ROLE_KIND] = df[ClassMetricVectorConstants.ROLE_KIND].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0e9d1680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COUNT_OF_ABSTRACT_METHODS                          int64\n",
       "COUNT_OF_FIELDS                                    int64\n",
       "COUNT_OF_INTERFACES                                int64\n",
       "COUNT_OF_METHODS                                   int64\n",
       "COUNT_OF_OBJECT_FIELDS                             int64\n",
       "COUNT_OF_OTHER_CLASSES_WITH_FIELD_OF_OWN_TYPE      int64\n",
       "COUNT_OF_OVERRIDDEN_METHODS                        int64\n",
       "COUNT_OF_PRIVATE_CONSTRUCTORS                      int64\n",
       "COUNT_OF_PRIVATE_FIELDS                            int64\n",
       "COUNT_OF_PRIVATE_METHODS                           int64\n",
       "COUNT_OF_STATIC_FIELDS                             int64\n",
       "COUNT_OF_STATIC_METHODS                            int64\n",
       "COUPLING_BETWEEN_OBJECTS                         float64\n",
       "DEPTH_OF_INHERITANCE                               int64\n",
       "design_pattern                                    object\n",
       "entity                                            object\n",
       "IS_ABSTRACT                                        int64\n",
       "IS_CLASS                                           int64\n",
       "IS_PUBLIC                                          int64\n",
       "IS_STATIC                                          int64\n",
       "LACK_OF_COHESION_OF_METHODS                      float64\n",
       "micro_architecture                                object\n",
       "project                                           object\n",
       "RESPONSE_FOR_A_CLASS                               int64\n",
       "role                                              object\n",
       "role_kind                                         object\n",
       "TIGHT_CLASS_COHESION                             float64\n",
       "WEIGHTED_METHOD_CLASS                              int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if columns in dataframe have expected types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f534e36",
   "metadata": {},
   "source": [
    "### Filter Dataframe entries by micro architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24e61d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_arches = df[ClassMetricVectorConstants.MICRO_ARCHITECTURE].unique().tolist()\n",
    "\n",
    "def view(micro_arch=''):\n",
    "    cols = [ClassMetricVectorConstants.ROLE_KIND, ClassMetricVectorConstants.ENTITY] + get_metric_columns()\n",
    "    display(df[df[ClassMetricVectorConstants.MICRO_ARCHITECTURE] == micro_arch]\n",
    "            [cols], clear=True)\n",
    "\n",
    "\n",
    "w = widgets.Dropdown(options=micro_arches)\n",
    "widgets.interactive(view, micro_arch=w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0616d25e-d4f0-4d42-91a2-6e33586a80e7",
   "metadata": {},
   "source": [
    "### Corelation Between Columns\n",
    "For each column we caclulate pairwaise the coefficient of corelation with other columns. The value of the coefficient can be interpreteted as:\n",
    "\n",
    "- between -1.0 and 0: Negative correlation; a increase in one column expects a decrease in the other; the lower the bigger the impact\n",
    "- equals 0: No correlation\n",
    "- between 0 and 1: Postive correlation; a increase in one column causes an increase the other; the higher the bigger the impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab51ef0-64b7-4858-ae32-20a1be678bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df[get_metric_columns()].copy()\n",
    "corr = df_corr.corr()\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        x=corr.columns,\n",
    "        y=corr.index,\n",
    "        z=np.array(corr),\n",
    "        text=corr.values,\n",
    "        texttemplate='%{text:.2f}'\n",
    "    )\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b016f497",
   "metadata": {},
   "source": [
    "### Distribution of roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79959eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = df.groupby([ClassMetricVectorConstants.ROLE]).size()\n",
    "temp = temp.sort_values(ascending=False).reset_index()\n",
    "px.bar(temp, x=ClassMetricVectorConstants.ROLE, y=0).update_layout(yaxis_title='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240b2766-9c37-4c29-a2d8-cee945d1ba17",
   "metadata": {},
   "source": [
    "### Distribution of design patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f654cb14-24d4-403d-ad90-7db5bbf0e1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binned_by_role = df.copy()\n",
    "df_binned_by_role = df_binned_by_role.drop_duplicates(\n",
    "    [ClassMetricVectorConstants.MICRO_ARCHITECTURE, ClassMetricVectorConstants.DESIGN_PATTERN]).reset_index()\n",
    "df_binned_by_role = df_binned_by_role[ClassMetricVectorConstants.DESIGN_PATTERN].value_counts(\n",
    ").reset_index()\n",
    "\n",
    "fig = px.histogram(df_binned_by_role, x=ClassMetricVectorConstants.DESIGN_PATTERN, y='count')\n",
    "fig.update_layout(xaxis_title='Design Pattern',\n",
    "                  yaxis_title='Count of Design Pattern')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca83e997",
   "metadata": {},
   "source": [
    "### Distribution for metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236b326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_histogram():\n",
    "    return go.Histogram(\n",
    "        histfunc='count',\n",
    "    )\n",
    "\n",
    "\n",
    "def update_histogram(figure: go.Figure, df: pd.DataFrame, metric: str):\n",
    "    figure['x'] = df[metric]\n",
    "\n",
    "\n",
    "generate_selectable_graph_for_metrics(\n",
    "    df, initial_histogram, update_histogram, 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e5650",
   "metadata": {},
   "source": [
    "### Box Plots for metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51e3449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_histogram():\n",
    "    return go.Box(\n",
    "    )\n",
    "\n",
    "\n",
    "def update_histogram(figure: go.Figure, df: pd.DataFrame, metric: str):\n",
    "    figure['x'] = df[metric]\n",
    "\n",
    "\n",
    "generate_selectable_graph_for_metrics(df, initial_histogram, update_histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa3c25",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "As design patterns can be considered as small scale appliances of software architecture, they consist of different entities with different relationships and roles to fulfill in the regarded design pattern. In order to detect design patterns, we first need to detect what kind of role a given Java class or entity it most likely corresponds to. To achieve this, machine learning model capable of classifying multiple labels should be considered. The extracted software metrics are the numerical inputs and the most likely roles in a design pattern are the result. \n",
    "As this falls in the area of supervised machine learning, initially the following models/techniques are to be considered:\n",
    "\n",
    "**NOTE:** This list is subject to change \n",
    "\n",
    "* Support Vector Machines\n",
    "* Tree Classifiers\n",
    "* Ensemble Classifiers (e.g Random Forest Classifier)\n",
    "* Custom Convoluted Network\n",
    "\n",
    "In order to optimize the given results of a given model, first RandomGridSearch is applied to determine a range of values or selection for the hyperparameters while GridSearch is used to determine the most optimal available value or selection for the regarded hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "edebf2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required import for machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import hpsklearn\n",
    "import hyperopt\n",
    "from dataclasses import dataclass, field\n",
    "import joblib\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import numpy as np\n",
    "from sklearn.base import clone\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from collections import defaultdict\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "065cf01b-0f73-41ad-bab3-fdffdd3775e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    train: pd.DataFrame\n",
    "    test: pd.DataFrame\n",
    "    label_col: List[str]\n",
    "    feature_cols: List[str]\n",
    "    #roleKindEncoder: LabelEncoder\n",
    "    roleEncoder: LabelEncoder\n",
    "    dataset: pd.DataFrame\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        df_copy = df.copy().dropna()\n",
    "        self.label_col = get_label_column()\n",
    "        self.feature_cols = get_metric_columns()\n",
    "        self.roleEncoder = LabelEncoder()\n",
    "        df_copy[ClassMetricVectorConstants.ROLE] = self.roleEncoder.fit_transform(df_copy[ClassMetricVectorConstants.ROLE])\n",
    "        df_copy = df_copy[self.label_col + self.feature_cols]\n",
    "        self.train, self.test = train_test_split(df_copy, test_size=0.15, stratify=df_copy[get_label_column()])\n",
    "\n",
    "    @classmethod\n",
    "    def top_k_design_patterns(cls, df: pd.DataFrame, k: int) -> \"Dataset\":\n",
    "        top = get_top_k_labels(df, k)\n",
    "        df_dataset = df[df[ClassMetricVectorConstants.DESIGN_PATTERN].isin(top)]\n",
    "        return cls(df_dataset)\n",
    "        \n",
    "\n",
    "    def get_X_train(self):\n",
    "        return self.train[self.feature_cols]\n",
    "    \n",
    "    def get_Y_train(self):\n",
    "        return self.train[self.label_col].values.ravel()\n",
    "\n",
    "    def get_X_test(self):\n",
    "        return self.test[self.feature_cols]\n",
    "\n",
    "    def get_Y_test(self):\n",
    "        return self.test[self.label_col].values.ravel()\n",
    "\n",
    "\n",
    "def get_top_k_labels(df: pd.DataFrame, k: int):\n",
    "    df_binned_by_role = df.copy()\n",
    "    df_binned_by_role = df_binned_by_role.drop_duplicates(\n",
    "        [ClassMetricVectorConstants.MICRO_ARCHITECTURE, ClassMetricVectorConstants.DESIGN_PATTERN])\n",
    "    df_binned_by_role = df_binned_by_role[ClassMetricVectorConstants.DESIGN_PATTERN].value_counts(\n",
    "    ).sort_values(ascending=False).head(k)\n",
    "    return df_binned_by_role.index.to_list()\n",
    "\n",
    "def scoring(target, pred):\n",
    "        return -f1_score(target, pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fdad0cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.top_k_design_patterns(df, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7df3789-9e1f-4ae3-8efd-6606839eaad0",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006c8ba1-fb69-4239-8f78-835b8a383c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_svm(dataset: Dataset):\n",
    "    X_train = dataset.get_X_train()\n",
    "    y_train = dataset.get_Y_train()\n",
    "    \n",
    "    X_test = dataset.get_X_test()\n",
    "    y_test = dataset.get_Y_test()\n",
    "\n",
    "    standard_scaler = StandardScaler()\n",
    "    X_train = standard_scaler.fit_transform(X_train)\n",
    "    X_test = standard_scaler.fit_transform(X_test)\n",
    "\n",
    "    svm_classifier = SVC(kernel='rbf', gamma=0.1, C=1.75)\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "    pred = svm_classifier.predict(X_test)\n",
    "    return svm_classifier.score(X_test, y_test)\n",
    "\n",
    "apply_svm(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0516a563-d27b-401d-ae1c-16bc4528c458",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395b288b-1501-4262-9a99-2711a575581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_random_forest(dataset: Dataset):\n",
    "    X_train = dataset.get_X_train()\n",
    "    y_train = dataset.get_Y_train()\n",
    "\n",
    "    X_test = dataset.get_X_test()\n",
    "    y_test = dataset.get_Y_test()\n",
    "\n",
    "    random_forest_classifier = RandomForestClassifier(\n",
    "        max_depth=30, random_state=1)\n",
    "    random_forest_classifier.fit(X_train, y_train)\n",
    "\n",
    "    pred = random_forest_classifier.predict(X_test)\n",
    "    return random_forest_classifier.score(X_test, y_test)\n",
    "\n",
    "\n",
    "apply_random_forest(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aa5f3c-7044-4840-9c10-b6a11691a5f0",
   "metadata": {},
   "source": [
    "### Get Best Possible Classifier with hyperopt-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16c407fe-985c-48c2-bbb7-8e5fe37f959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_hyperopt(dataset: Dataset, evals: int = 10):\n",
    "    \n",
    "    \n",
    "    X_train = dataset.get_X_train()\n",
    "    y_train = dataset.get_Y_train()\n",
    "\n",
    "    X_test = dataset.get_X_test()\n",
    "    y_test = dataset.get_Y_test()\n",
    "\n",
    "    \n",
    "    chosen_classifiers = [\n",
    "        hpsklearn.random_forest_classifier('random_forest'),\n",
    "        hpsklearn.k_neighbors_classifier('knn'),\n",
    "        hpsklearn.svc('svm')\n",
    "    ]\n",
    "\n",
    "    p = 1 / len(chosen_classifiers)\n",
    "    classifiers = hyperopt.hp.pchoice('cls', [(p, c) for c in chosen_classifiers])\n",
    "\n",
    "    hyper_estimator = hpsklearn.HyperoptEstimator(\n",
    "        classifier=classifiers,\n",
    "        preprocessing=[],\n",
    "        max_evals=evals,\n",
    "        algo=hyperopt.tpe.suggest,\n",
    "        trial_timeout=180,\n",
    "        loss_fn=scoring\n",
    "        \n",
    "    )\n",
    "\n",
    "    hyper_estimator.fit(X_train, y_train)\n",
    "    best_model = hyper_estimator.best_model()['learner']\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return hyper_estimator.score(X_test, y_test), best_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "470b4886-38fc-4331-bb96-a7dcb12c2986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                 | 0/1 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 27.80trial/s, best loss: 0.72]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 26.29trial/s, best loss: 0.72]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 39.85trial/s, best loss: 0.6]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 40.40trial/s, best loss: 0.5800000000000001]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.81trial/s, best loss: 0.5800000000000001]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 35.34trial/s, best loss: 0.5800000000000001]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 33.53trial/s, best loss: 0.5800000000000001]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 10.01trial/s, best loss: 0.45999999999999996]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 40.58trial/s, best loss: 0.45999999999999996]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 19.61trial/s, best loss: 0.45999999999999996]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 40.54trial/s, best loss: 0.45999999999999996]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:00<00:00, 17.03trial/s, best loss: 0.45999999999999996]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 40.40trial/s, best loss: 0.45999999999999996]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:00<00:00, 30.03trial/s, best loss: 0.45999999999999996]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00,  3.45trial/s, best loss: 0.45999999999999996]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 15.48trial/s, best loss: 0.45999999999999996]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:00<00:00, 31.04trial/s, best loss: 0.45999999999999996]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [00:00<00:00, 34.87trial/s, best loss: 0.45999999999999996]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 27.83trial/s, best loss: 0.45999999999999996]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 39.49trial/s, best loss: 0.45999999999999996]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.67      0.50      0.57         4\n",
      "           2       0.60      0.75      0.67         4\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.73      1.00      0.84         8\n",
      "           5       0.50      0.25      0.33         4\n",
      "           6       0.50      0.67      0.57         9\n",
      "           7       1.00      0.50      0.67         4\n",
      "           8       1.00      1.00      1.00         1\n",
      "           9       0.17      0.50      0.25         2\n",
      "          10       1.00      0.50      0.67         2\n",
      "          11       0.00      0.00      0.00         1\n",
      "          12       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.58        45\n",
      "   macro avg       0.55      0.47      0.48        45\n",
      "weighted avg       0.59      0.58      0.56        45\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'HyperOpt-Score: 0.5777777777777777 Mean Cross Validation Score: [0.3898914  0.34082978 0.44039061]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_best_iteration(df: pd.DataFrame, top_k: int, max_evals: int, k_split: int):\n",
    "    dataset = Dataset.top_k_design_patterns(df, top_k)\n",
    "    score, estimator = apply_hyperopt(dataset, evals=max_evals)\n",
    "    #unfitted_estimator = clone(estimator)\n",
    "    cross_score = cross_val_score(estimator, dataset.get_X_train(), dataset.get_Y_train(), cv=k_split, scoring='f1_weighted')\n",
    "    joblib.dump(estimator, 'estimater.joblib')\n",
    "    return f'HyperOpt-Score: {score} Mean Cross Validation Score: {cross_score}'\n",
    "    \n",
    "get_best_iteration(df, 4, 20, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6f516cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adapter', 'Adapter', 'Adapter', 'Adapter', 'Command', 'Command', 'Command', 'Command', 'Observer', 'Observer', 'Observer', 'Observer', 'Singleton', 'Singleton', 'Singleton', 'Singleton']\n",
      "['Adapter', 'Command', 'Adapter', 'Adapter', 'Adapter', 'Adapter', 'Adapter', 'Singleton', 'Command', 'Observer', 'Observer', 'Command', 'Singleton', 'Singleton', 'Singleton', 'Singleton']\n",
      "Entire Process\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Adapter       0.50      0.75      0.60         4\n",
      "     Command       0.00      0.00      0.00         4\n",
      "    Observer       1.00      0.50      0.67         4\n",
      "   Singleton       0.80      1.00      0.89         4\n",
      "\n",
      "    accuracy                           0.56        16\n",
      "   macro avg       0.57      0.56      0.54        16\n",
      "weighted avg       0.57      0.56      0.54        16\n",
      "\n",
      "Model\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "         adaptee       0.67      0.67      0.67         3\n",
      "         adapter       1.00      1.00      1.00         4\n",
      "          client       1.00      0.75      0.86         4\n",
      "         command       1.00      1.00      1.00         1\n",
      " concretecommand       0.80      0.50      0.62         8\n",
      "concreteobserver       0.67      1.00      0.80         4\n",
      " concretesubject       1.00      0.56      0.71         9\n",
      "         invoker       1.00      1.00      1.00         4\n",
      "        observer       1.00      1.00      1.00         1\n",
      "        receiver       0.29      1.00      0.44         2\n",
      "       singleton       1.00      1.00      1.00         2\n",
      "         subject       0.50      1.00      0.67         1\n",
      "          target       1.00      1.00      1.00         2\n",
      "\n",
      "        accuracy                           0.78        45\n",
      "       macro avg       0.84      0.88      0.83        45\n",
      "    weighted avg       0.87      0.78      0.79        45\n",
      "\n",
      "Params\n",
      "{'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': None, 'max_features': 'log2', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 105, 'n_jobs': 1, 'oob_score': False, 'random_state': 0, 'verbose': False, 'warm_start': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/memi/Dokumente/master_thesis/project/.venv/lib/python3.10/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class DpRole:\n",
    "    role_name: str\n",
    "    mutliple_occurences: bool = field(default=False)\n",
    "\n",
    "#Singelton-, Adapter-, Command- und Observer-Entwurfsmuster \n",
    "design_patterns_to_detect = {\n",
    "    'Singleton': [DpRole('singleton')],\n",
    "    'Adapter': [DpRole('target'), DpRole('client', True), DpRole('adaptee', True), DpRole('adapter', True)],\n",
    "    'Command': [DpRole('command'), DpRole('concreteCommand', True), DpRole('client'), DpRole('invoker'), DpRole('receiver')],\n",
    "    'Observer': [DpRole('subject'), DpRole('observer'), DpRole('concreteSubject'), DpRole('concreteObserver', True)]\n",
    "}\n",
    "\n",
    "top_patterns = get_top_k_labels(df, 4)\n",
    "d = df[df[ClassMetricVectorConstants.DESIGN_PATTERN].isin(top_patterns)].reset_index()\n",
    "random_micro_arch = d.groupby([ClassMetricVectorConstants.DESIGN_PATTERN]).sample(4)[ClassMetricVectorConstants.MICRO_ARCHITECTURE].to_list()\n",
    "\n",
    "vectors = []\n",
    "for m in random_micro_arch:\n",
    "    v = df[df[ClassMetricVectorConstants.MICRO_ARCHITECTURE] == m].copy().reset_index()\n",
    "    vectors.append(v)\n",
    "\n",
    "model = joblib.load('estimater.joblib')\n",
    "\n",
    "def match_pattern(role_encoder: LabelEncoder, roles_df: pd.DataFrame, dps: dict):\n",
    "    # X = roles_df[get_metric_columns()]\n",
    "    # y = model.predict(X)\n",
    "    # roles = role_encoder.inverse_transform(y.ravel())\n",
    "    # scores = defaultdict(int)\n",
    "    # for dp, dp_roles in dps.items():\n",
    "    #     single_roles = set()\n",
    "    #     role_freq = defaultdict(int)\n",
    "    #     for role in roles:\n",
    "    #         dp_role = None\n",
    "    #         for d in dp_roles:\n",
    "    #             if d.role_name == role:\n",
    "    #                 dp_role = d\n",
    "    #                 break\n",
    "    #         if dp_role and not dp_role.mutliple_occurences and dp_role.role_name not in single_roles:\n",
    "    #             role_freq[dp_role.role_name] += 1\n",
    "    #             single_roles.add(dp_role.role_name)\n",
    "    #         elif dp_role and dp_role.role_name not in single_roles and dp_role.mutliple_occurences:\n",
    "    #             role_freq[dp_role.role_name] += 1\n",
    "    #         else:\n",
    "    #             role_freq[role] = 0\n",
    "    #     score = sum([freq for freq in role_freq.values()])/len(roles)\n",
    "    #     scores[dp] = score\n",
    "    # return scores\n",
    "\n",
    "    X = roles_df[get_metric_columns()]\n",
    "    y = model.predict(X)\n",
    "    roles = role_encoder.inverse_transform(y.ravel())\n",
    "    scores = defaultdict(float)\n",
    "\n",
    "    predicted_role_freq = defaultdict(int)\n",
    "    for role in roles:\n",
    "        predicted_role_freq[role] += 1\n",
    "\n",
    "    for dp, dp_roles in dps.items():\n",
    "        matched_roles = defaultdict(int)\n",
    "\n",
    "        dp_roles_dict = {d.role_name: d for d in dp_roles}\n",
    "\n",
    "        for role in predicted_role_freq:\n",
    "            if role in dp_roles_dict:\n",
    "                dp_role = dp_roles_dict[role]\n",
    "                if (dp_role.mutliple_occurences or predicted_role_freq[role] == 1) and matched_roles[role] < predicted_role_freq[role]:\n",
    "                    matched_roles[role] += 1\n",
    "\n",
    "        total_possible_matches = sum([1 for d in dp_roles if d.mutliple_occurences or d.role_name in predicted_role_freq])\n",
    "        score = sum(matched_roles.values()) / total_possible_matches if total_possible_matches > 0 else 0\n",
    "        scores[dp] = score\n",
    "\n",
    "    return scores\n",
    "\n",
    "labels = []\n",
    "preds = []\n",
    "for v in vectors:\n",
    "    label = v[ClassMetricVectorConstants.DESIGN_PATTERN][0]\n",
    "    labels.append(label)\n",
    "    predictions = match_pattern(dataset.roleEncoder, v, design_patterns_to_detect)\n",
    "    p = max(predictions.items(), key=lambda x: x[1])\n",
    "    preds.append(p[0])\n",
    "\n",
    "print(labels)\n",
    "print(preds)\n",
    "print('Entire Process')\n",
    "print(classification_report(labels, preds))\n",
    "\n",
    "print('Model')\n",
    "predictions = model.predict(dataset.get_X_test())\n",
    "predictions = dataset.roleEncoder.inverse_transform(predictions)\n",
    "real_labels = dataset.roleEncoder.inverse_transform(dataset.get_Y_test())\n",
    "print(classification_report(real_labels, predictions))\n",
    "\n",
    "print(\"Params for Random Forest\")\n",
    "print(model.get_params())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
